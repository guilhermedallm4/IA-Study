{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "Hau-byUsv_6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "32OTvIA3v_N8"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key = 'x')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt Engineer\n",
        "É o processo de refinir e austar esse texto de entrada para obter respostas mais precisas e úteis do modelo. Isso envolve escolher as palavras certas, o tom e o contexto que direcionam o modelo apra a saída desejada. Uma boa engenharia de prompt pode melhorar significativamente a qualidade e relevância das respostas geradas pelo modelo.\n",
        "\n",
        "##Componentes Básicos\n",
        "\n",
        "###Persona\n",
        "\n",
        "É um perfil ficticio que representa um conjunto específico de características, comportamentes e estilos de resposta que são atribuídos ao modelo. Ao interagir com usuários, a persona determina como o modelo de linguagem \"Se comporta\", influenciando seu tom, formalidade, linguagem e até mesmo o tipo de conhecimento que parece possuir. A definição de uma persona ajuda a criar uma experiência de interação mais coesa e previsivel, tornando o LLM mais eficas e agradável para o usuário final.\n",
        "\n",
        "###Descrição da Tarefa\n",
        "\n",
        "Orienta o modelo sobre o modelo sobr eo objetivo específico da interação. Isso inclui o tipo de informação ou ação deseada, como resolver um rpoblema matemático ou gerar um texto criativo. Com instruções precisas, o modelo pode focar melhor no seu propósito e gerar respostas mais relevantes e úteis. Juntos, a persoan e a descrição da tarefa audam a moldar a interação de forma que alinha as capacidades do modelo com as necessidades do usuário\n",
        "\n",
        "Exemplo de prompt:\n",
        "Sua tarefa é reecrever uma sentença. Para executar a tarefa com sucesso, siga os 2 passos abaixo:\n",
        "1) Corrija erros gramaticais, ortografias e de pontuação.\n",
        "2) Remova redundâncias.\n",
        "\n",
        "Sentença: ODASDASJDASDJAS\n",
        "\n",
        "###Few-shot Learning\n",
        "\n",
        "É uma abordagem onde um LLM aprende a realizar uma tarefa a partir de um número muito limitado de exemplos de treinamento, isso significa que o modelo pode entender e realizar uma nova tarefa após ver alguns exemplos dela.\n",
        "\n",
        "Na engenharia de prompt, few-shot learning é importante porque permite que os usuários \"ensinem\" o modelo a realizar tarefas específicas sem a necessidade de grandes conuntos de dados de treinamento. Ao incluir alguns exemplos de entrada e saídas desejadas no prompt, os usuários podem guiar o modelo para gerar respostas no mesmo formato ou estilo, melhorando a eficácia e a precisão do modelo em tarefas específicas com apenas uma pequena quantidade de orientação."
      ],
      "metadata": {
        "id": "HUyaSNrTPB8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persona = 'Você é um especialista em X do setor tal'\n",
        "\n",
        "prompt = 'Texto e indicativo da tarefa'\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        #Persona\n",
        "        {'role': 'system', 'content': persona},\n",
        "        #Prompt\n",
        "        {'role': 'user', 'content': prompt}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "9HijkhDyCE4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "uqVYrNSoCYhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Quantidade de tokens do GPT"
      ],
      "metadata": {
        "id": "TPDXCrq_PWV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frase = \"Isso é uma frase\"\n",
        "\n",
        "n_palavras = len(frase.split())\n",
        "\n",
        "n_tokens = int(n_palavras/0.75)\n",
        "\n",
        "print(f\"Tokens do GPT: {n_tokens} Tokens\")\n",
        "\n",
        "n_tokens_output = int(20/0.75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upp-RF-qERly",
        "outputId": "1f783568-6339-4c32-c726-16c9f06c1c55"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens do GPT: 5 Tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custo por Token GPT"
      ],
      "metadata": {
        "id": "f0gAA255PmnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custo_por_token = {\n",
        "    'gpt-3.5-turbo': {\n",
        "        'input': 0.001,\n",
        "        'output': 0.002\n",
        "    }\n",
        "}\n",
        "\n",
        "custo_input_token = 5*(custo_por_token.get('gpt-3.5-turbo').get('input')*n_tokens)/1000\n",
        "\n",
        "custo_output_token = 5*(custo_por_token.get('gpt-3.5-turbo').get('output')*n_tokens_output)/1000"
      ],
      "metadata": {
        "id": "e-MmqfwcPmr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Custo total da tarefa: {custo_input_token + custo_output_token}\")"
      ],
      "metadata": {
        "id": "bi2oJ2RjQGIV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}